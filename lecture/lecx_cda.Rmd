---
title: 'Lec 03: Categorical Data Analysis'
date: "Last Updated `r Sys.time()`"
output:
  html_document:
    highlight: tango
    theme: flatly
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r,echo=FALSE, warning=FALSE, message=FALSE}
library(knitr); library(rmarkdown); library(dplyr)
library(xtable); library(ggplot2); library(scales)
opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE) 
```

# Introduction

This set of lecture notes uses data on incoming emails for the first three months of 2012 for David Diezâ€™s (An Open Intro Statistics Textbook author) Gmail Account, early months of 2012. All personally identifiable information has been removed.

```{r}
email <- read.delim("https://norcalbiostat.netlify.com/data/email.txt", header=TRUE, sep="\t")
```

The research question of interest is 
> Does including numbers in an email increase or decrease the chance the email is flagged as spam? 

or more generally, 

> Is there an association between numbers in an email and the email being flagged as spam? 

\newpage

# Data setup

### Response Variable

The response variable is whether or not the email is flagged as spam. In the data set originally this variable is listed as

* `spam` 0/1 binary indicator if a an email is flagged as spam

For plotting, and for some analyses we need to have a categorical version of this variable. So we create a new variable `spam_cat` that is a categorical (factor) version of spam with levels Ham and Spam.

```{r}
email$spam_cat <- factor(email$spam, labels=c("Ham", "Spam"))
table(email$spam, email$spam_cat, useNA="always")
```

### Explanatory Variable

The explanatory variable `number` measures the size of the number contained in the email. It is a factor variable with three levels: 

* `none`: No numbers 
* `small`: Only values under 1 million
* `big`: A value of 1 million or more

```{r}
table(email$number)
```

It may not be the size that matters, but whether or not there any number at all. So let's make a binary version of this variable, both as numeric 0/1 and factor Yes/No. 

* `hasnum`: 0/1 binary indicator for if the email contains any sized number
* `hasnum_at`: Categorical (factor) version of `hasnum`: Yes/No

```{r}
email$hasnum  <- ifelse(email$number %in% c("big", "small"), 1, 0)
email$hasnum_cat <- ifelse(email$number %in% c("big", "small"), "Yes", "No")
table(email$hasnum_cat, email$number, useNA="always")
```

\newpage

# Exploratory Data Analsysis 

Looking at the frequency and column percent tables we see that

* 27.1% (149/549) of emails without numbers are flagged as spam
* 6.5% (218/3372) of emails with numbers in them are flagged as spam. 

```{r}
tab.spam.hasnum <- table(email$spam_cat, email$hasnum_cat,dnn=c("Spam", "Has Number"))
addmargins(tab.spam.hasnum)
round(prop.table(tab.spam.hasnum,margin=2)*100,1)
```

```{r, fig.height=4, fig.width=5}
df.table <- data.frame(prop.table(tab.spam.hasnum, margin=2))
plot.spam <- filter(df.table, Spam=="Spam")

ggplot(plot.spam, aes(x=Has.Number, y=Freq)) + 
        geom_col(position = position_dodge()) + theme_bw() + 
        geom_text(aes(y=Freq+.04, label=paste0(round(Freq,2)*100, "%")), 
                      position = position_dodge(width=1)) + 
        labs(x="Has a Number", y="%", 
             title="Proportion of emails flagged as spam") + 
        scale_y_continuous(limits=c(0,1), labels=percent)
```

\newpage

# Chi-Squared Distribution
Much of categorical data analysis uses the $\chi^{2}$ distribution. It is a probability distribution, but it has a different shape compared to the Normal. It looks a little bit like an F-distribution. 

```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=4}
x <- seq(0,10, by=.01)
df <- c(1,2,3,4,6,8)
plot(c(0,10), c(0,1), type="n", xlab="x", ylab="P(X=x)", axes=FALSE, 
     main="Chi-Squared PDF")
for(k in df){
  lines(x, dchisq(x, k), col=k)
}
legend("topright", col=df, lty=1, legend=df, ncol=2, title="df")
axis(1); axis(2, las=2); box()
```

* The shape is controlled by a degrees of freedom parameter (df)
* Is used in many statistical tests for categorical data. 
* Is always positive (it's squared!)
    - High numbers result in low p-values
* Mathematically connected to many other distributions
    * Special case of the gamma distribution (One of the most commonly used 
      statistical distributions)
    * The sample variance has a $\chi^{2}_{n-1}$ distribution.  
    * The sum of $k$ independent standard normal distributions has a $\chi^{2}_{k}$ distribution. 
    * The ANOVA F-statistic is the ratio of two $\chi^{2}$ distributions divided by 
      their respective degrees of freedom. 

\newpage

# Difference of two proportions

Now let's consider comparisons of proportions in two independent samples. 

**Ex**: Comparison of proportions of head injuries sustained in auto 
accidents by passengers wearing seat belts to those not wearing seat belts. 
You may have already guessed the form of the estimate: $\hat{p}_1 - \hat{p}_2$.


### Example 1: Do numbers in emails affect rate of spam? 

If we look at the rate of spam for emails with and without numbers, 
we see that 6% of emails with numbers are flagged as spam compared to 
27% of emails without numbers are flagged as spam. 


This is such a large difference that we don't really _need_ a statistical
test to tell us that this difference is significant. 
But we will do so anyhow for examples sake. 

1. **State the research question:** Are emails that contain numbers more likely to be spam? 
2. **Define your parameters:**   
Let $p_{nonum}$ be the proportion of emails _without_ numbers that are flagged as spam.  
Let $p_{hasnum}$ be the proportion of emails _with_ numbers that are flagged as spam.  

3. **Set up your statistical hypothesis:**  
$H_{0}: p_{nonum} = p_{hasnum}$   
$H_{A}: p_{nonum} \neq p_{hasnum}$ 

4. **Check assumptions:** Use the pooled proportion $\hat{p} = 367/3921 = .094$ to check the success-failure condition. 
```{r}
p.hat <- mean(email$spam)
```

* $\hat{p}*n_{nonum}$ = `p.hat * sum(email$hasnum==0)` = `r p.hat * sum(email$hasnum==0)`
* $\hat{p}*n_{hasnum}$ = `p.hat * sum(email$hasnum==1)` = `r p.hat * sum(email$hasnum==1)`
* $(1-\hat{p})*n_{nonum}$ = `(1-p.hat)* sum(email$hasnum==0)` = `r (1-p.hat)* sum(email$hasnum==0)`
* $(1-\hat{p})*n_{hasnum}$ = `(1-p.hat)* sum(email$hasnum==1)` = `r (1-p.hat) * sum(email$hasnum==1)`

The success-failure condition is satisfied since all values are at least 10, and we can
safely apply the normal model.

5. **Test the hypothesis** by calculating a test statistic and corresponding p-value. 
   Interpret the results in context of the problem. 
```{r}
chisq.test(email$spam_cat, email$hasnum_cat)
```

Significantly more emails with numbers were flagged as spam compared to emails without numbers (27.1% versus 6.4% , p<.0001). 


\newpage

# Tests of Association

There are three main tests of association for $rxc$ contingency table, where $r$ is the number of rows and indexed by $i$, and $c$ is the number of columns and indexed by $j$. 

* Test of Goodness of Fit
* Tests of Independence
* Test of Homogeneity

<br></br>


## Goodness of Fit

* OpenIntro Statistics: Chapter 6.3
* Tests whether a set of multinomial counts is distributed according to a 
  theoretical set of population proportions.
* Does a set of categorical data come from a claimed distribution? 
* Are the observed frequencies consistent with theory? 

$H_{0}$: The data come from the claimed discrete distribution  
$H_{A}$: The data to not come from the claimed discrete distribution. 

<br></br>

## Test of Independence

* OpenIntro Statistics: Chapter 6.4
* Determine whether two categorical variables are associated 
  with one another in the population
      - Ex. Race and smoking, or education level and political affiliation. 
* Data are collected at random from a population and the two 
  categorical variables are observed on each unit. 

$H_{0}: p_{ij} = p_{i.}p_{.j}$   
$H_{A}: p_{ij} \neq p_{i.}p_{.j}$ 

<br></br>

## Test of Homogeneity

* A test of homogeneity tests whether two (or more) sets of multinomial 
  counts come from different sets of population proportions.
* Does two or more sub-groups of a population share the same distribution
  of a single categorical variable?
    - Ex: Do people of different races have the same proportion of smokers?
    - Ex: Do different education levels have different proportions of 
      Democrats, Republicans, and Independents?
* Data on one characteristic is collected from randomly sampling 
  individuals within each subroup of the second characteristic. 

$H_{0}:$ 

$$p_{11} = p_{12} = \ldots = p_{1c}$$
$$p_{21} = p_{22} = \ldots = p_{2c}$$
$$\qquad \vdots $$
$$ p_{r1} = p_{r2} = \ldots = p_{rc}$$


$H_{A}:$ At least one of the above statements is false. 


<br></br>


All three tests use the **Pearsons' Chi-Square** test statistic. 

<br></br>

## Pearsons' Chi-Square
The chi-squared test statistic is the sum of the squared differences 
between the observed and expected values, divided by the expected value. 

### One way table
$$\chi^{2} = \sum^{r}_{i=1} \frac{(O_{i}-E_{i})^2}{E_{i}}$$

* $O_{i}$ observed number of type $i$
* $E_{i}$ expected number of type $i$. Equal to $Np_{i}$ under the null hypothesis
* N is the total sample size
* df = r-1

### Two way tables
$$\chi^{2} = \sum^{r}_{i=1}\sum^{c}_{j=1} \frac{(O_{ij}-E_{ij})^2}{E_{ij}}$$

* $O_{ij}$ observed number in cell $ij$
* $E_{ij} = Np_{i.}p_{.j}$ under the null hypothesis
* N is the total sample size
* df = (r-1)(c-1)

<br></br>


### Conducting these tests in R. 
* Test of equal or given proportions using `prop.test()`
```{r}
prop.test(table(email$number, email$spam))
```

* Chi-squared contingency table tests and goodness-of-fit tests using `chisq.test()`.  
```{r}
chisq.test(email$number, email$spam)
```

* `prop.test` 
    - has a similar output appearance to other hypothesis tests 
    - shows sample proportions of outcome within each group
* `chisq.test`
    - stores the matrices of $O_{ij}$, $E_{ij}$, the residuals and standardized residuals

```{r}
results.table <- chisq.test(email$number, email$spam)
results.table$expected
```

### Conducting these tests in SPSS

https://libguides.library.kent.edu/SPSS/ChiSquare


### Mosaicplots

* The Pearson $\chi^{2}$ test statistic = Sum of squared residuals. 
* A shaded mosaic plot shows the magnitude of the residuals. 
    * Blue (positive residuals) = More frequent than expected
    * Red (negative residuals) = Less frequent than expected. 

```{r}
tab <- table(email$number, email$spam)
mosaicplot(tab, shade=TRUE, main="Association of spam status and number size in emails")
```

There are more spam emails with no numbers, fewer Ham emails with no numbers, 
and fewer spam emails with small numbers than would be expected
if these factors were independent. 
  
* More information on mosaicplots - http://www.datavis.ca/online/mosaics/about.html


# Assumptions and Extensions

* Simple random sample
* Adequate expected cell counts
    - At least 5 in all cells of a 2x2, or at least 80% of cells in a larger table.
    - NO cells with 0 cell count
* Observations are independent

If one or more of these assumptions are not satisfied, other methods
may still be useful. 

* McNemarâ€™s Test for paired or correlated data
* Fishers exact test for when cell sizes are small (<5-10)
* Inter-rater reliability: Concordant and Discordant Pairs


